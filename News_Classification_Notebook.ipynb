{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc44bf5",
   "metadata": {},
   "source": [
    "# News Text Classification Pipeline with spaCy & scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70966b3",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1. Introduction](#1-introduction)\n",
    "- [2. Importing Required Libraries](#2-importing-required-libraries)\n",
    "- [3. Loading and Exploring the Dataset](#3-loading-and-exploring-the-dataset)\n",
    "  - [3.1 Importing the Dataset from Kaggle](#31-importing-the-dataset-from-kaggle)\n",
    "  - [3.2 Loading the Dataset](#32-loading-the-dataset)\n",
    "  - [3.3 Checking for Missing Values](#33-checking-for-missing-values)\n",
    "  - [3.4 Balancing the Dataset](#34-balancing-the-dataset)\n",
    "- [4. Feature Extraction with spaCy](#4-feature-extraction-with-spacy)\n",
    "  - [4.1 Importing Spacy Large Language model](#41-importing-spacy-large-language-model)\n",
    "  - [4.2 Vectorizing the text column](#42-vectorizing-the-text-column)\n",
    "- [5. Model Training and Evaluation](#5-model-training-and-evaluation)\n",
    "  - [5.1 Splitting the Data](#51-splitting-the-data)\n",
    "  - [5.2 Training the Model](#52-training-the-model)\n",
    "  - [5.3 Model Prediction](#53-model-prediction)\n",
    "  - [5.4 Model Evaluation](#54-model-evaluation)\n",
    "- [6. Model Selection and Hyperparameter Tuning](#6-model-selection-and-hyperparameter-tuning)\n",
    "  - [6.1 Setting Up and Running GridSearchCV](#61-setting-up-and-running-gridsearchcv)\n",
    "  - [6.2 Analyzing Grid Search Results](#62-analyzing-grid-search-results)\n",
    "  - [6.3 Extracting Best Results for Each Classifier](#63-extracting-best-results-for-each-classifier)\n",
    "- [7. Final Model Evaluation](#7-final-model-evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b5217",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook provides a comprehensive guide to classifying news articles as Fake or Real using modern machine learning techniques. We walk through each step of the workflow, including data loading, preprocessing, feature extraction with spaCy, model training, evaluation, and hyperparameter tuning. By the end, you will have a clear understanding of how to build and assess a robust news classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e6f5b5",
   "metadata": {},
   "source": [
    "## 2. Importing Required Libraries\n",
    "\n",
    "We begin by importing all the necessary libraries for data manipulation, visualization, and machine learning. Libraries like pandas and numpy help with data handling, matplotlib and seaborn are used for visualization, and scikit-learn provides tools for model building and evaluation. We also use spaCy for advanced natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4905d-742b-4578-8bdb-a79a3c80848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Natural Language Processing\n",
    "import spacy\n",
    "\n",
    "# File and Directory operations\n",
    "import os\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04ba75-703f-46d1-82b3-a2111259f262",
   "metadata": {},
   "source": [
    "## 3. Loading and Exploring the Dataset\n",
    "\n",
    "In this section, we load the news dataset and perform initial exploration to understand its structure, check for missing values, and get a sense of the data distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a905d4",
   "metadata": {},
   "source": [
    "### 3.1 Importing the Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41215e93",
   "metadata": {},
   "source": [
    "This code shell below automates downloading and extracting a news dataset from Kaggle:\n",
    "\n",
    "1. **Creates a directory** called `NEWS DATA` if it doesn't exist.\n",
    "2. **Configures Kaggle API credentials** by copying `kaggle.json` to the correct location (`~/.kaggle`) and setting secure permissions.\n",
    "3. **Downloads the dataset** (`fake-and-real-news-dataset`) from Kaggle into the `NEWS DATA` directory using the Kaggle CLI.\n",
    "4. **Unzips the downloaded dataset** into the `NEWS DATA` directory for further use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure NEWS DATA directory exists\n",
    "os.makedirs(\"NEWS DATA\", exist_ok=True)\n",
    "\n",
    "# Set Kaggle API credentials (assumes kaggle.json is in the current directory)\n",
    "kaggle_json_path = os.path.expanduser(\"kaggle.json\")\n",
    "kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "if not os.path.exists(os.path.join(kaggle_dir, \"kaggle.json\")):\n",
    "    shutil.copy(kaggle_json_path, os.path.join(kaggle_dir, \"kaggle.json\"))\n",
    "\n",
    "os.chmod(os.path.join(kaggle_dir, \"kaggle.json\"), 0o600)\n",
    "\n",
    "# Download and unzip dataset to NEWS DATA directory\n",
    "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset -p \"NEWS DATA\"\n",
    "\n",
    "with zipfile.ZipFile(\"NEWS DATA/fake-and-real-news-dataset.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"NEWS DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef4fc5",
   "metadata": {},
   "source": [
    "### 3.2 Loading the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ff314",
   "metadata": {},
   "source": [
    "Dataset separated in two files:\n",
    "- `Fake.csv` (23502 fake news article)\n",
    "- `True.csv` (21417 true news article)\n",
    "\n",
    "Dataset columns:\n",
    "\n",
    "- `Title`: title of news article\n",
    "- `Text`: body text of news article\n",
    "- `Subject`: subject of news article\n",
    "- `Date`: publish date of news article\n",
    "\n",
    "From each dataset, we need to extract the `Text` column for the classification task. For each dataset, we will create a new column `label` to indicate whether the news is fake or real. The `Fake.csv` will have `label` as `Fake` and `True.csv` will have `label` as `Real`. After this we will concatenate both datasets into a single DataFrame. We also need to shuffle the DataFrame to ensure that the data is randomly distributed. The final dataset is then exported to a CSV file named `news_data.csv` for further processing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = pd.read_csv(\"NEWS DATA/True.csv\")\n",
    "real_data[\"label\"] = \"Real\"\n",
    "real_df = real_data[[\"text\", \"label\"]]\n",
    "\n",
    "fake_data = pd.read_csv(\"NEWS DATA/Fake.csv\")\n",
    "fake_data[\"label\"] = \"Fake\"\n",
    "fake_df = fake_data[[\"text\", \"label\"]]\n",
    "\n",
    "real_fake_df = pd.concat([real_df, fake_df], axis=0)\n",
    "news_data = real_fake_df.sample(frac=1).reset_index(drop=True)\n",
    "news_data.to_csv(\"NEWS DATA/news_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabb345",
   "metadata": {},
   "source": [
    "We load the news data from a CSV file. The first few rows are displayed to get an overview of the data structure and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7b2e7-21d8-43a3-8421-a73345e84e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(\"NEWS DATA/news_data.csv\")\n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38faf30-3e96-4374-b961-375bd3bdaa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef73c64",
   "metadata": {},
   "source": [
    "### 3.3 Checking for Missing Values\n",
    "\n",
    "It is important to check for missing or empty values in the dataset to ensure data quality. Here, we check for nulls and empty strings in the 'text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da322435-3241-469e-b11d-d12058056b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce95523-6cd2-43fe-9d36-a647116274c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"text\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd9db2f",
   "metadata": {},
   "source": [
    "We found that there are no null values, but there are some empty strings. We will exclude these from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54fb727-642e-4e59-80b1-bae102a896a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_text = news_data[news_data[\"text\"] == \" \"]\n",
    "print(empty_text.shape)\n",
    "empty_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa374def-2824-4204-8246-53411b224cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_text[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e16f6",
   "metadata": {},
   "source": [
    "The final dataset will be the one with no empty strings in the 'text' column, which we will use for further processing and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_final = news_data[~(news_data[\"text\"] == \" \")]\n",
    "new_data_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_final[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b9e8f3",
   "metadata": {},
   "source": [
    "### 3.4 Balancing the Dataset\n",
    "\n",
    "To ensure fair model training, we balance the dataset by sampling an equal number of examples from each class. This helps prevent bias towards the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = (\n",
    "    new_data_final.groupby(\"label\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(5000, random_state=2024))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13942fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c0f93",
   "metadata": {},
   "source": [
    "Mapping the labels to integers is done to facilitate model training. The 'Fake' label is mapped to 0 and 'Real' to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d523411",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"label\"] = news_df[\"label\"].map({\"Fake\": 0, \"Real\": 1})\n",
    "news_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_df.loc[0, \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364a51e",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction with spaCy\n",
    "\n",
    "We use spaCy's large English language model to convert news text into numerical vectors. These vectors capture semantic information from the text, which can be used as features for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c397a56",
   "metadata": {},
   "source": [
    "### 4.1 Importing Spacy Large Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c638e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = news_df.iloc[0, 0]\n",
    "text_vector = nlp(text).vector\n",
    "print(text_vector.shape)\n",
    "text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc8ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_word_list = []\n",
    "for word in nlp(text):\n",
    "    lemma_word_list.append(word.lemma_)\n",
    "\n",
    "\n",
    "print(text)\n",
    "print(\" \".join(lemma_word_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9fd69",
   "metadata": {},
   "source": [
    "### 4.2 Vectorizing the `text` column\n",
    "\n",
    "Now, we convert each news article's text into a vector using spaCy. This allows us to use these vectors as features for our machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"text_vector\"] = news_df[\"text\"].apply(lambda x: nlp(x).vector)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966cf8ef",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "We split the data into training and test sets, then train a Multinomial Naive Bayes classifier using the extracted features. The model's performance is evaluated using accuracy, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb426c2",
   "metadata": {},
   "source": [
    "### 5.1 Splitting the Data\n",
    "\n",
    "Splitting the dataset into training and test sets is crucial for evaluating the model's performance. We use an 80-20 split, where 80% of the data is used for training and 20% for testing. We have also used stratified sampling to ensure that both sets have a balanced representation of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    news_df[\"text_vector\"].values,\n",
    "    news_df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=2024,\n",
    "    stratify=news_df[\"label\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33400601",
   "metadata": {},
   "source": [
    "Stacking with `np.stack()` is used to combine a sequence of arrays (like lists or 1D arrays) into a single NumPy array with an extra dimension. In your code:\n",
    "\n",
    "```python\n",
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d = np.stack(X_test)\n",
    "```\n",
    "\n",
    "**Why is this required?**\n",
    "\n",
    "- **Consistent Shape:** Machine learning models (especially in scikit-learn, TensorFlow, or PyTorch) expect input data as a 2D array: shape `(num_samples, num_features)`.\n",
    "- **List to Array:** If `X_train` is a list of 1D arrays (each representing a sample), stacking turns it into a 2D array where each row is a sample.\n",
    "- **Efficient Computation:** NumPy arrays are faster and more memory-efficient than Python lists for numerical operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c01f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2d = np.stack(X_train)\n",
    "X_test_2d = np.stack(X_test)\n",
    "print(\n",
    "    f\"Shape of training data before stacking: {X_train.shape}, after stacking: {X_train_2d.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72c600",
   "metadata": {},
   "source": [
    "### 5.2 Training the Model\n",
    "\n",
    "We have trained a Multinomial Naive Bayes classifier on the training set. The training data is also scaled using MinMaxScaler to ensure that all features contribute equally to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"clf\", MultinomialNB())])\n",
    "pipe.fit(X_train_2d, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00483f68",
   "metadata": {},
   "source": [
    "### 5.3 Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cb3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test_2d)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "print(f\"The accuracy score of the classifier is {acc_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ceab7",
   "metadata": {},
   "source": [
    "### 5.4 Model Evaluation\n",
    "\n",
    "We evaluate the model's performance using accuracy, confusion matrix, and classification report. The accuracy score gives us a quick overview of how well the model is performing, while the confusion matrix and classification report provide detailed insights into the model's predictions across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbf56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, columns=news_data[\"label\"].unique(), index=news_data[\"label\"].unique()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\")\n",
    "plt.title(\n",
    "    \"Confusion Matrix for the Predicted Classes\\n\"\n",
    "    f\"Classifier: MultinomialNB() ; Accuracy: {acc_score * 100:.2f}%\",\n",
    "    fontsize=14,\n",
    "    pad=10,\n",
    ")\n",
    "plt.xlabel(\"True Label\")\n",
    "plt.ylabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c27eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "report_df = pd.DataFrame(report).transpose().fillna(0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(report_df.iloc[:-1, :-1], annot=True, cmap=\"Blues\")\n",
    "plt.title(\"Classification Report Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed112e",
   "metadata": {},
   "source": [
    "## 6. Model Selection and Hyperparameter Tuning\n",
    "\n",
    "To find the best performing model, we use GridSearchCV to test different classifiers and hyperparameters. This helps us identify the optimal model configuration for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894e2cc",
   "metadata": {},
   "source": [
    "### 6.1. Setting Up and Running GridSearchCV\n",
    "\n",
    "Here, we set up GridSearchCV with the pipeline and parameter grid defined above. GridSearchCV performs cross-validation to evaluate all combinations of classifiers and hyperparameters, helping us find the best model configuration based on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\n",
    "            \"clf\",\n",
    "            MultinomialNB(),\n",
    "        ),  # Placeholder, to be replaced by different classifiers\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = [\n",
    "    {\n",
    "        \"clf\": [MultinomialNB()],\n",
    "        \"clf__alpha\": [0.1, 0.5, 1.0],  # Example hyperparameters for MultinomialNB\n",
    "    },\n",
    "    {\n",
    "        \"clf\": [RandomForestClassifier()],\n",
    "        \"clf__n_estimators\": [100, 200],\n",
    "        \"clf__max_depth\": [None, 10, 20],\n",
    "    },\n",
    "    {\"clf\": [SVC()], \"clf__C\": [0.1, 1, 10], \"clf__kernel\": [\"linear\", \"rbf\"]},\n",
    "]\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the data\n",
    "grid_search.fit(X_train_2d, y_train)\n",
    "\n",
    "# Display the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dabef72",
   "metadata": {},
   "source": [
    "### 6.2 Analyzing Grid Search Results\n",
    "\n",
    "This cell extracts the results from GridSearchCV and creates a DataFrame to display the mean and standard deviation of test scores for each parameter combination. The results are sorted to highlight the best-performing configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "\n",
    "# Create a DataFrame with the relevant information\n",
    "df_results = pd.DataFrame(\n",
    "    {\n",
    "        \"mean_test_score\": results[\"mean_test_score\"],\n",
    "        \"std_test_score\": results[\"std_test_score\"],\n",
    "        \"params\": results[\"params\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the mean test score in descending order\n",
    "df_results = df_results.sort_values(by=\"mean_test_score\", ascending=False)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df_results.to_csv(\"RESULTS/grid_search_results.csv\", index=False)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f3be5",
   "metadata": {},
   "source": [
    "### 6.3 Extracting Best Results for Each Classifier\n",
    "\n",
    "After this, we loop through the results of GridSearchCV to find and display the best accuracy score and corresponding parameters for each classifier type (MultinomialNB, RandomForestClassifier, SVC). This helps us compare the top-performing models side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "\n",
    "# Initialize an empty list to store the best results\n",
    "best_results = []\n",
    "\n",
    "# Loop through the parameter grid to find the best results for each classifier\n",
    "for classifier in [\"MultinomialNB\", \"RandomForestClassifier\", \"SVC\"]:\n",
    "    # Filter results for the current classifier\n",
    "    classifier_results = [\n",
    "        (mean_score, params)\n",
    "        for mean_score, params in zip(results[\"mean_test_score\"], results[\"params\"])\n",
    "        if params[\"clf\"].__class__.__name__ == classifier\n",
    "    ]\n",
    "\n",
    "    # Get the best score and corresponding parameters\n",
    "    if classifier_results:\n",
    "        best_score, best_params = max(classifier_results, key=lambda x: x[0])\n",
    "        best_results.append(\n",
    "            {\n",
    "                \"classifier\": classifier,\n",
    "                \"best_accuracy_score\": best_score,\n",
    "                \"best_params\": best_params,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_best_results = pd.DataFrame(best_results).sort_values(\n",
    "    by=\"best_accuracy_score\", ascending=False\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df_best_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e2f72",
   "metadata": {},
   "source": [
    "## 7. Final Model Evaluation\n",
    "\n",
    "After selecting the best model and hyperparameters, we retrain the model and evaluate its performance on the test set. This gives us a realistic estimate of how well the model will perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536dadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_SVC = Pipeline([(\"sclaer\", MinMaxScaler()), (\"clf\", SVC(kernel=\"rbf\", C=10))])\n",
    "\n",
    "pipe_SVC.fit(X_train_2d, y_train)\n",
    "y_pred_SVC = pipe_SVC.predict(X_test_2d)\n",
    "acc_score_SVC = accuracy_score(y_test, y_pred_SVC)\n",
    "\n",
    "print(f\"The accuracy score of the SVM model is {acc_score_SVC * 100:.2f}%\\n\")\n",
    "\n",
    "print(f\"The classification report:\\n {classification_report(y_test, y_pred_SVC)}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_SVC)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, columns=news_data[\"label\"].unique(), index=news_data[\"label\"].unique()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\")\n",
    "plt.title(\n",
    "    \"Confusion Matrix for the Predicted Classes\\n\"\n",
    "    f\"Classifier: SVC() ; Accuracy: {acc_score_SVC * 100:.2f}%\",\n",
    "    fontsize=14,\n",
    "    pad=10,\n",
    ")\n",
    "plt.xlabel(\"True Label\")\n",
    "plt.ylabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
